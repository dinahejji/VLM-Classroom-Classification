<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Multimodal Classification of Classroom Scenes Using Vision-Language Models" />
  <title>Classroom Scene Analysis and Robot Interaction Using Vision-Language Models</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Playfair+Display:wght@600&display=swap" rel="stylesheet"/>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      background-color: #f9f9f9;
      color: #1f2937;
      font-family: 'Inter', sans-serif;
      line-height: 1.75;
      padding: 40px 20px;
    }
    header {
      text-align: center;
      margin-bottom: 50px;
    }
    header h1 {
      font-family: 'Playfair Display', serif;
      font-size: 2.8rem;
      font-weight: 600;
      color: #111827;
      max-width: 900px;
      margin: auto;
      line-height: 1.4;
    }
    section {
      max-width: 900px;
      margin: auto;
      padding: 30px;
      background-color: #ffffff;
      border-radius: 12px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.04);
    }
    section p {
      text-align: justify;
      margin-bottom: 20px;
      font-size: 1.05rem;
    }
    h2 {
      margin-top: 40px;
      font-size: 1.5rem;
      font-weight: 600;
      color: #111827;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 5px;
    }
    ul {
      margin-top: 15px;
      padding-left: 20px;
    }
    li {
      margin-bottom: 8px;
    }
    pre {
      background-color: #f3f4f6;
      padding: 15px;
      border-radius: 8px;
      font-size: 0.95rem;
      overflow-x: auto;
    }
    a {
      color: #2563eb;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    iframe {
      border: none;
      border-radius: 10px;
      margin-top: 15px;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin-top: 20px;
    }
    footer {
      text-align: center;
      margin-top: 60px;
      color: #9ca3af;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Classroom Scene Analysis and Robot Interaction Using Vision-Language Models</h1>
  </header>

  <section>
<h2>Project Overview</h2>
<p>
  This project introduces a multimodal AI system designed to understand and respond to real classroom scenes by combining vision, language, and behavioral cues. At its core, a fine-tuned Vision-Language Model (VLM) analyzes classroom images and transcripts to classify the type of activityâ€”such as normal lecture, flipped learning, lab session, or examâ€”and to detect the academic topic being discussed.
</p>
<p>
  In parallel, a behavior recognition module based on LSTM processes keypoints extracted from pose estimation to track student engagement over time. This allows the system to identify behaviors like writing, using a mobile phone, or disengagement.
</p>
<p>
  Both the VLM's scene understanding and the LSTM's behavioral insights are fused into a unified prompt sent to a Large Language Model (LLM), which then generates intelligent, context-aware robot actions. The result is a fully integrated pipeline for classroom observation and autonomous human-robot interaction.
</p>
<p>
  This end-to-end pipeline enables scalable, context-aware observation of real-world educational environments and supports intelligent classroom agents capable of proactive, multimodal assistance.
</p>



<h2>Methodology</h2>
<p>
  The system follows a modular, multimodal pipeline composed of three main AI components:
</p>

<ul>
  <li><strong>1. Vision-Language Model (VLM):</strong> A fine-tuned model that classifies the classroom activity type (e.g., flipped, lab, exam) and infers the academic topic by analyzing both the classroom image and its associated audio transcript.</li>

  <li><strong>2. Behavior Analysis using LSTM:</strong> Pose estimation is applied to classroom videos to extract keypoints for each student. These time-series keypoint sequences are passed to a trained LSTM model to detect engagement-related behaviors such as writing, reading, phone usage, or inattention.</li>

  <li><strong>3. Robot Action Generation via LLM:</strong> Outputs from the VLM and LSTM are fused into a structured prompt. This prompt is sent to a Large Language Model (LLM), which generates an adaptive responseâ€”providing a scene summary and suggesting robot actions tailored to specific student behaviors.</li>
</ul>

<p>
  This architecture allows the system to not only classify scenes but to interpret them meaningfully and act upon them, enabling intelligent, context-aware robot interaction in real educational settings.
</p>

 <img src="https://raw.githubusercontent.com/dinahejji/VLM-Classroom-Classification/main/arch.png" alt="System Architecture Diagram">


    <h2>Results</h2>
    <ul>
      <li>Keypoints-based pose estimation used to train LSTM for student behavior recognition</li>
      <li>LSTM for behavior detection from time-series visual patterns</li>
      <li>Fine-tuned multimodal VLM for scene understanding using image + classroom audio transcript</li>
      <li>Human Robot Interaction Suggestions by LLMs based on Multi-Fusion Scene Understanding from the outputs of both VLM and LSTM</li>
    </ul>



<h2>Sample LLM Prompt for Robot Interaction</h2>
<p>
  The following prompt is sent to a Large Language Model (LLM) to generate intelligent robot actions based on fused outputs from both the Vision-Language Model (VLM) and LSTM-based behavior analysis:
</p>
<pre>
You are an intelligent classroom support robot.

ðŸ§  VLM Scene Understanding:
- Type: Flipped Classroom
- Topic: Kinematics and Velocity-Time Graphs

ðŸŽ¯ LSTM Student Behaviors:
- Student 1: Writing_On_Textbook
- Student 2: Not Looking at Board
- Student 3: Holding_Mobile_Phone

Please generate:
- A brief scene summary
- Targeted robot intervention actions per student
</pre>


    <h2>Demo</h2>
    <p>Watch a demo of the VLM classifying a real classroom scene using a video + transcript input:</p>
    <iframe 
      src="https://drive.google.com/file/d/13kfb5S9BogH2ae0E-fVQ4IFG_g0hMpKx/preview" 
      width="100%" height="400" allow="autoplay" allowfullscreen>
    </iframe>

<h2>Status</h2>
<ul>
  <li>âœ… Built a labeled dataset of classroom scenes with class types: <em>hybrid</em>, <em>normal</em>, <em>active</em>, <em>flipped</em>, <em>lab</em>, and <em>exam</em></li>
  <li>âœ… Fine-tuned <code>LLaMA-3.2-11B-Vision-Instruct</code> to classify classroom activity and detect the current academic topic from images and transcripts</li>
  <li>âœ… Used Detectron2 for pose estimation and trained an LSTM model for student behavior recognition with 90% accuracy on the EduNet dataset</li>
  <li>ðŸ”„ Currently integrating VLM and LSTM outputs into a unified prompt for LLM-based robot action generation</li>
</ul>

  </section>

  <footer>
    <p>Â© 2025 Dina Hejji Â· <a href="https://github.com/dinahejji" target="_blank">GitHub Profile</a></p>
  </footer>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Multimodal Classification of Classroom Scenes Using Vision-Language Models" />
  <title>Classroom Scene Analysis and Robot Interaction Using Vision-Language Models</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Playfair+Display:wght@600&display=swap" rel="stylesheet"/>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      background-color: #f9f9f9;
      color: #1f2937;
      font-family: 'Inter', sans-serif;
      line-height: 1.75;
      padding: 40px 20px;
    }
    header {
      text-align: center;
      margin-bottom: 50px;
    }
    header h1 {
      font-family: 'Playfair Display', serif;
      font-size: 2.8rem;
      font-weight: 600;
      color: #111827;
      max-width: 900px;
      margin: auto;
      line-height: 1.4;
    }
    section {
      max-width: 900px;
      margin: auto;
      padding: 30px;
      background-color: #ffffff;
      border-radius: 12px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.04);
    }
    section p {
      text-align: justify;
      margin-bottom: 20px;
      font-size: 1.05rem;
    }
    h2 {
      margin-top: 40px;
      font-size: 1.5rem;
      font-weight: 600;
      color: #111827;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 5px;
    }
    ul {
      margin-top: 15px;
      padding-left: 20px;
    }
    li {
      margin-bottom: 8px;
    }
    pre {
      background-color: #f3f4f6;
      padding: 15px;
      border-radius: 8px;
      font-size: 0.95rem;
      overflow-x: auto;
    }
    a {
      color: #2563eb;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    iframe {
      border: none;
      border-radius: 10px;
      margin-top: 15px;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin-top: 20px;
    }
    footer {
      text-align: center;
      margin-top: 60px;
      color: #9ca3af;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Classroom Scene Analysis and Robot Interaction Using Vision-Language Models</h1>
  </header>

  <section>
<h2>Project Overview</h2>
<p>
  This project presents a multimodal AI system that understands classroom scenes by combining visual and auditory information. Using a fine-tuned Vision-Language Model (VLM), the system classifies the type of classroom activityâ€”such as normal lecture, flipped learning, lab session, or examâ€”and identifies the ongoing academic topic based on classroom images and audio transcripts.
</p>
<p>
  To enhance the system's interpretability, an LSTM-based behavior analysis module detects student engagement patterns from visual sequences, adding a behavioral dimension to scene understanding. These outputs are then fused into a unified representation and passed to a Large Language Model (LLM), which generates autonomous human-robot interaction suggestions tailored to the classroom context.
</p>
<p>
  This end-to-end pipeline enables scalable, context-aware observation of real-world educational environments and supports intelligent classroom agents capable of proactive, multimodal assistance.
</p>


    <h2>Repository</h2>
    <p>
      GitHub: <a href="https://github.com/dinahejji/VLM-Classroom-Classification" target="_blank">
        github.com/dinahejji/VLM-Classroom-Classification</a>
    </p>

    <h2>Methodology</h2>
    <p>
      The system architecture integrates multiple AI components: a fine-tuned Vision-Language Model (VLM) for classroom scene understanding, an LSTM for analyzing student behavior, and a Large Language Model (LLM) to generate human-robot interaction suggestions based on a fused multimodal understanding.
    </p>

    <h2>Results</h2>
    <ul>
      <li>Human Robot Interaction Suggestions by LLMs based on Multi-Fusion Scene Understanding</li>
      <li>LSTM for behavior detection from time-series visual patterns</li>
      <li>Fine-tuned multimodal VLM for scene understanding using image + classroom audio transcript</li>
    </ul>

    <h2>Sample Prompt</h2>
    <pre>
### Instruction:
You are a classroom observer. Based on the image and the following transcript (automatically generated from classroom audio), classify:
1. The type of classroom activity (e.g., normal lecture, flipped classroom, lab session, exam, etc.)
2. The academic topic being discussed.

### Transcript (converted from audio):
"Okay, so what does this graph represent? It's the velocity-time graph for motion..."
    </pre>

    <h2>Demo</h2>
    <p>Watch a demo of the VLM classifying a real classroom scene using a video + transcript input:</p>
    <iframe 
      src="https://drive.google.com/file/d/1jTh8a13Kq8JxVrTm-94Kt0l0ZeTDwYmw/preview" 
      width="100%" height="400" allow="autoplay" allowfullscreen>
    </iframe>

    <h2>Status</h2>
    <ul>
      <li>âœ… Created a dataset of classroom scenes labeled with class types: hybrid, normal, active, flipped, lab, and exam</li>
      <li>âœ… Fine-tuned <code>LLaMA-3.2-11B-Vision-Instruct</code> to extract class type and current topic from image + transcript</li>
      <li>âœ… Trained an LSTM model for student behavior detection with 90% accuracy on a labeled dataset</li>
      <li>ðŸ”„ Currently working on the multimodal fusion module and LLM-based prompt/action generation</li>
    </ul>
  </section>

  <footer>
    <p>Â© 2025 Dina Hejji Â· <a href="https://github.com/dinahejji" target="_blank">GitHub Profile</a></p>
  </footer>
</body>
</html>

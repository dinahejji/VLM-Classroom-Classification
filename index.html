<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Multimodal Classification of Classroom Scenes Using Vision-Language Models" />
  <title>Multimodal Classification of Classroom Scenes Using VLMs</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Playfair+Display:wght@600&display=swap" rel="stylesheet"/>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      background-color: #f9f9f9;
      color: #1f2937;
      font-family: 'Inter', sans-serif;
      line-height: 1.75;
      padding: 40px 20px;
    }
    header {
      text-align: center;
      margin-bottom: 50px;
    }
    header h1 {
      font-family: 'Playfair Display', serif;
      font-size: 2.8rem;
      font-weight: 600;
      color: #111827;
      max-width: 900px;
      margin: auto;
      line-height: 1.4;
    }
    section {
      max-width: 900px;
      margin: auto;
      padding: 30px;
      background-color: #ffffff;
      border-radius: 12px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.04);
    }
    section p {
      text-align: justify;
      margin-bottom: 20px;
      font-size: 1.05rem;
    }
    h2 {
      margin-top: 40px;
      font-size: 1.5rem;
      font-weight: 600;
      color: #111827;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 5px;
    }
    ul {
      margin-top: 15px;
      padding-left: 20px;
    }
    li {
      margin-bottom: 8px;
    }
    pre {
      background-color: #f3f4f6;
      padding: 15px;
      border-radius: 8px;
      font-size: 0.95rem;
      overflow-x: auto;
    }
    a {
      color: #2563eb;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    iframe {
      border: none;
      border-radius: 10px;
      margin-top: 15px;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin-top: 20px;
    }
    footer {
      text-align: center;
      margin-top: 60px;
      color: #9ca3af;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Multimodal Classification of Classroom Scenes Using Vision-Language Models</h1>
  </header>

  <section>
    <h2>Project Overview</h2>
    <p>
      This work presents a system that classifies classroom scenes based on both visual and textual data using fine-tuned Vision-Language Models (VLMs). It identifies the type of classroom activityâ€”such as lectures, labs, or active learning sessionsâ€”as well as the academic topic being discussed.
    </p>
    <p>
      Manual annotation of educational environments is time-consuming and subjective, while traditional classification models struggle to capture the complex, multimodal nature of real classroom interactions. An automated approach can support large-scale educational analytics and adaptive learning systems.
    </p>
    <p>
      A VLM is intended to be fine-tuned using a custom dataset of annotated classroom images paired with transcripts of student-teacher dialogue. The model jointly processes visual cues and spoken content to infer both pedagogical style and subject matter.
    </p>
    <p>
      The system demonstrates the potential of multimodal AI in understanding complex human-centered settings like education.
    </p>

    <h2>Repository</h2>
    <p>
      GitHub: <a href="https://github.com/dinahejji/VLM-Classroom-Classification" target="_blank">
        github.com/dinahejji/VLM-Classroom-Classification</a>
    </p>

    <h2>Methodology</h2>
    <p>
      The system architecture integrates multiple AI components: a fine-tuned Vision-Language Model (VLM) for classroom scene understanding, an LSTM for analyzing student behavior, and a Large Language Model (LLM) to generate human-robot interaction suggestions based on a fused multimodal understanding.
    </p>
<iframe 
  src="https://i.imgur.com/Xx3rLUe.png" 
  width="100%" 
  height="500" 
  style="border: none; background: transparent;" 
  loading="lazy">
</iframe>

    <h2>Results</h2>
    <ul>
      <li>Human Robot Interaction Suggestions by LLMs based on Multi-Fusion Scene Understanding</li>
      <li>LSTM for behavior detection from time-series visual patterns</li>
      <li>Fine-tuned multimodal VLM for scene understanding using image + classroom audio transcript</li>
    </ul>

    <h2>Sample Prompt</h2>
    <pre>
### Instruction:
You are a classroom observer. Based on the image and the following transcript (automatically generated from classroom audio), classify:
1. The type of classroom activity (e.g., normal lecture, flipped classroom, lab session, exam, etc.)
2. The academic topic being discussed.

### Transcript (converted from audio):
"Okay, so what does this graph represent? It's the velocity-time graph for motion..."
    </pre>

    <h2>Demo</h2>
    <p>Watch a demo of the VLM classifying a real classroom scene using a video + transcript input:</p>
    <iframe 
      src="https://drive.google.com/file/d/1jTh8a13Kq8JxVrTm-94Kt0l0ZeTDwYmw/preview" 
      width="100%" height="400" allow="autoplay" allowfullscreen>
    </iframe>

    <h2>Status</h2>
    <ul>
      <li>âœ… Created a dataset of classroom scenes labeled with class types: hybrid, normal, active, flipped, lab, and exam</li>
      <li>âœ… Fine-tuned <code>LLaMA-3.2-11B-Vision-Instruct</code> to extract class type and current topic from image + transcript</li>
      <li>âœ… Trained an LSTM model for student behavior detection with 90% accuracy on a labeled dataset</li>
      <li>ðŸ”„ Currently working on the multimodal fusion module and LLM-based prompt/action generation</li>
    </ul>
  </section>

  <footer>
    <p>Â© 2025 Dina Hejji Â· <a href="https://github.com/dinahejji" target="_blank">GitHub Profile</a></p>
  </footer>
</body>
</html>

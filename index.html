<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Multimodal Classification of Classroom Scenes Using Vision-Language Models" />
  <title>Multimodal Classroom Understanding and Robotic Action Generation via Vision and Large Language Models</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Playfair+Display:wght@600&display=swap" rel="stylesheet"/>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      background-color: #f9f9f9;
      color: #1f2937;
      font-family: 'Inter', sans-serif;
      line-height: 1.75;
      padding: 40px 20px;
    }
    header {
      text-align: center;
      margin-bottom: 50px;
    }
    header h1 {
      font-family: 'Playfair Display', serif;
      font-size: 2.8rem;
      font-weight: 600;
      color: #111827;
      max-width: 900px;
      margin: auto;
      line-height: 1.4;
    }
    section {
      max-width: 900px;
      margin: auto;
      padding: 30px;
      background-color: #ffffff;
      border-radius: 12px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.04);
    }
    section p {
      text-align: justify;
      margin-bottom: 20px;
      font-size: 1.05rem;
    }
    h2 {
      margin-top: 40px;
      font-size: 1.5rem;
      font-weight: 600;
      color: #111827;
      border-bottom: 2px solid #e5e7eb;
      padding-bottom: 5px;
    }
    ul {
      margin-top: 15px;
      padding-left: 20px;
    }
    li {
      margin-bottom: 8px;
    }
    pre {
      background-color: #f3f4f6;
      padding: 15px;
      border-radius: 8px;
      font-size: 0.95rem;
      overflow-x: auto;
    }
    a {
      color: #2563eb;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    iframe {
      border: none;
      border-radius: 10px;
      margin-top: 15px;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin-top: 20px;
    }
    footer {
      text-align: center;
      margin-top: 60px;
      color: #9ca3af;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <header>
    <h1>Multimodal Classroom Understanding and Robotic Action Generation via Vision and Large Language Models</h1>
  </header>

  <section>
    <h2>Robotic Perception Course Project (May 2025) - by:</h2>
<p>
Dina Hejji</p>
<h2>Project Overview</h2>
<p>
This project introduces a multimodal framework designed to understand classroom scenes and students behavior to generate robotic
actions. The framework combines vision, language, and behavioral cues to generate context-aware robotic interactions for an in-
class assistant robot. At its core, a fine-tuned Vision-Language Model (VLM) analyzes classroom images and transcripts to classify
the type of activity such as normal lecture, hybrid lecture, active learning lecture, flipped learning, lab session, or exam, and to
detect the academic topic being discussed in the classroom</p>
<p>
In parallel, a behavior recognition module based on Long Short-Term Memory (LSTM) processes keypoints extracted from pose
estimation to track student behavior over time. This allows the system to identify behaviors like writing, reading, or using a mobile
phone</p>
<p>
Both the VLM’s scene understanding and the LSTM’s behavioral insights are fused into a unified prompt sent to a Large Language
Model (LLM), which then generates intelligent, context-aware robot actions. The result is a fully integrated pipeline for classroom
observation and autonomous human-robot interaction.</p>
<p>
This end-to-end pipeline enables scalable, context-aware observation of real-world educational environments and supports intelli-
gent classroom agents capable of proactive, context-aware assistance.</p>



<h2>Methodology</h2>
<p>
  The system follows a modular, multimodal pipeline composed of three main AI components:
</p>

<ul>
  <li><strong>1. Vision-Language Model (VLM):</strong> A fine-tuned model that classifies the classroom activity type (e.g., flipped classroom, lab lecture, exam,active learning lecture, normal lecture, hybrid lecture) and infers the academic topic by analyzing both the classroom image and its associated audio transcript.</li>

  <li><strong>2. Behavior Analysis using LSTM:</strong> Pose estimation using Detectron2 is applied to classroom videos to extract keypoints for each student. These time-series keypoint sequences are passed to a trained LSTM model to detect engagement-related behaviors such as writing on textbook, writing on board, reading, holding a book, holding mobile phone, sitting on desk.</li>

  <li><strong>3. Robot Action Generation via LLM:</strong> Outputs from the VLM and LSTM are fused into a structured prompt. This prompt is sent to a Large Language Model (LLM), which generates an adaptive response—providing a scene summary and suggesting robot actions tailored to specific student behaviors.</li>
</ul>

<p>
  This architecture allows the system to not only classify scenes but to interpret them meaningfully and act upon them, enabling intelligent, context-aware robot interaction in real educational settings.
</p>

 <img src="https://raw.githubusercontent.com/dinahejji/VLM-Classroom-Classification/main/arch.png" alt="System Architecture Diagram">

<h2 id="results">Results</h2>
<ul>
  <li><strong>Pose Estimation + LSTM:</strong> Student behavior classification on EduNet-v1 dataset achieved <strong>92.23%</strong> accuracy using Detectron2 keypoints and a trained LSTM model.</li>

  <li><strong>Vision-Language Models (VLM):</strong> Fine-tuned <code>Qwen2.5-7B</code> model achieved <strong>93.52%</strong> accuracy for topic classification and <strong>76.01%</strong> for classroom type classification on a custom 625-sample multimodal dataset.</li>

  <li><strong>LLM Action Validity:</strong> The Large Language Model (LLM) generated robot actions with <strong>95%</strong> rule-based DAG validity and <strong>86%</strong> G-Eval appropriateness score across all test scenarios.</li>

  <li><strong>LLMs on Behavior Classification:</strong> General-purpose LLMs performed poorly on behavior recognition: GPT o4-mini scored <strong>25.19%</strong>, and DeepSeek R1 scored <strong>23.41%</strong>, reinforcing the importance of the dedicated LSTM model.</li>
</ul>



    <h2>Prompt Formats</h2>
<p>
  The prompts used for fine-tuning the Vision-Language Model (VLM) and guiding the Large Language Model (LLM) are structured to ensure clarity, consistency, and context-awareness.
</p>

<h3>VLM Fine-tuning Prompt (LLaVA, LLaMA, Qwen)</h3>
<p>
  The VLM receives a prompt that includes a classroom scene image and an accompanying transcript. This prompt is structured in JSON format to simulate instruction-tuned dialogue data.
</p>
<pre>
{
  "messages": [
    {
      "role": "user", 
      "content": [
        {
          "type": "text", 
          "text": "&lt;instruction with transcript&gt;"
        },
        {
          "type": "image", 
          "image": "&lt;image_path&gt;"
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "### Image Description: ...\n### Class: ...\n### Topic: ..."
        }
      ]
    }
  ]
}
</pre>

<h3>LLM Prompt Format for Robot Action Generation</h3>
<p>
  To ensure meaningful and context-appropriate robotic responses, the LLM is guided using a structured prompt that follows the CISCO pattern: <strong>Context, Intent, Situation, Constraint, Output</strong>.
</p>
<pre>
You are an intelligent classroom support robot tasked
with monitoring student behavior and classroom dynamics. 
Your role is to observe all students collectively and
take one appropriate action only when necessary.

You must analyze the behavior of all students provided 
in the input and consider the class type (e.g., Lecture,
Group Work, Exam, etc.).

Based on this full-context understanding, you should 
suggest one action only from the following list that
best supports the learning environment:
Encourage Focus, Support with Topic, No Action Needed,
Ask a Follow-up Question, Suggest Collaboration,
Offer Encouragement, Invite Participation, Clarify
Misunderstanding, Remind of Task or Goal, Warning.

Choose your action carefully and intervene only if the 
situation requires support or redirection. Otherwise,
remain passive.

Scene Understanding:
&lt;vlm_output&gt;

Student Behavior:
&lt;lstm_output&gt;

Class Type:
&lt;class_type&gt;

Topic (if applicable):
&lt;topic&gt;

Please respond in the format below:

Summary: &lt;1-2 lines summarizing the scene and student 
behavior&gt;
Action Type: &lt;Choose exactly one from the list above&gt;
Robot Action: &lt;1-2 line helpful suggestion&gt;
</pre>


    <h2>Full Demo Video</h2>
    <p>Below is a live demo showcasing the full pipeline: from classroom scene classification to intelligent robot suggestions.</p>
    <iframe 
      src="https://drive.google.com/file/d/13kfb5S9BogH2ae0E-fVQ4IFG_g0hMpKx/preview" 
      width="100%" height="400" allow="autoplay" allowfullscreen>
    </iframe>


        <h2>Full GUI Demo</h2>
    <p>Below is a live demo showcasing the full pipeline: from classroom scene classification to intelligent robot suggestions.</p>
    <iframe 
      src="https://drive.google.com/file/d/1wGe0e7NbnZxvJcJN1CfHehnd32foHtRS/preview" 
      width="100%" height="400" allow="autoplay" allowfullscreen>
    </iframe>

  </section>

  <footer>
    <p>© 2025 Dina Hejji · <a href="https://github.com/dinahejji" target="_blank">GitHub Profile</a></p>
  </footer>
</body>
</html>
